{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f087cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://realpython.github.io/fake-jobs/'\n",
    "\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9069f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01be02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56cac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02c8b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find('h2').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca20c8f",
   "metadata": {},
   "source": [
    "b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8918e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs =soup.findAll('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3cbb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs = jobs\n",
    "print(type(jobs))\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f60321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_job = jobs[0]\n",
    "print(type(first_job))\n",
    "first_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f5c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles  = [x.text for x in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825232e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f1a85",
   "metadata": {},
   "source": [
    "c. Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb5b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "body_html = str(soup.find('body'))\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(body_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations =soup.findAll('p', attrs ={'class':'location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadec49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [x.text for x in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d938de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [x.strip() for x in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies =soup.findAll('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0621109",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [x.text for x in companies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517732e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates =soup.find('time').text\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc297f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = soup.findAll('time')\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed28b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = [x.text for x in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f7208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_python_jobs = pd.DataFrame(list(zip(companies, locations, dates)), columns=['companies', 'locations', 'dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1222ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_link = soup.findAll('a', string=\"Apply\")\n",
    "apply_link\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0b1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apply_link = [x.get('href')for x in apply_link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e900229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apply_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_python_jobs = pd.DataFrame(list(zip(companies, locations, dates, apply_link)), columns=['companies', 'locations', 'dates', 'apply_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a873d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_python_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response2 = requests.get('https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(response2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51053dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = soup.find('div', attrs = {'class':'content'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789504c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = fake_python_jobs.apply_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptions():\n",
    "    response2 = requests.get(url)\n",
    "    description = soup.find('div', attrs = {'class':'content'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525864c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_python_jobs['apply_link'].apply(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d246ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
